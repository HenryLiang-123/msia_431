{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('full_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('financial_analysis').getOrCreate()\n",
    "df = spark.read.csv('full_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df.withColumn('bar_range', F.floor((df.bar_num - 1) / 10)) # starts from 1\n",
    "df = df.withColumn('bar_range', df.bar_range.cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg, lag, col\n",
    "# Calculate average profit per bar_range and trade_id\n",
    "df_range_avg = df.groupBy(\"trade_id\", \"bar_range\").agg(avg(\"profit\").alias(\"avg_profit\"))\n",
    "\n",
    "# Define a window partitioned by trade_id and ordered by bar_range\n",
    "window = Window.partitionBy('trade_id').orderBy('bar_range')\n",
    "\n",
    "# Create profit_lag column, which is the lagged cumulative average of avg_profit\n",
    "df_range_avg = df_range_avg.withColumn('cumulative_avg_profit', avg('avg_profit').over(window))\n",
    "df_range_avg = df_range_avg.withColumn('profit_lag', lag('cumulative_avg_profit').over(window))\n",
    "\n",
    "df_range_avg.orderBy('trade_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profit_lag = avg of avg_profit of bar_ranges before current\n",
    "# avg_profit = avg of profit of current bar_range\n",
    "df_new = df.join(df_range_avg, ['trade_id', 'bar_range'], 'left')\n",
    "df_new.select('trade_id', 'bar_range', 'profit_lag', 'avg_profit', 'profit').orderBy(['trade_id', 'bar_range']).show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('financial_analysis').getOrCreate()\n",
    "df = spark.read.csv('full_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df.withColumn('bar_range', F.floor((df.bar_num - 1) / 10)) # starts from 1\n",
    "df = df.withColumn('bar_range', df.bar_range.cast(\"int\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "window = Window.partitionBy('trade_id', 'bar_range').orderBy('time_stamp').rowsBetween(Window.unboundedPreceding, -1)\n",
    "df = df.withColumn('profit_lag', avg(df['profit']).over(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('trade_id', 'bar_range').agg(avg('profit').alias('avg_profit')).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the window by 'trade_id', ordered by 'bar_num'\n",
    "window = Window.partitionBy('trade_id').orderBy('bar_num')\n",
    "\n",
    "# Use the lag function to get the past 10 profits, then add these as new features\n",
    "for i in range(1, 11):\n",
    "    df = df.withColumn(f'profit_lag_{i}', lag(df.profit, count=i).over(window))\n",
    "\n",
    "# Fill the null values (since the lagged values for the first 10 bars will be null)\n",
    "df = df.na.fill(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
