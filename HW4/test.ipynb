{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('financial_analysis').getOrCreate()\n",
    "df = spark.read.csv('full_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df.withColumn('bar_range', F.floor((df.bar_num - 1) / 10)) # starts from 1\n",
    "df = df.withColumn('bar_range', df.bar_range.cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg, lag, col\n",
    "# Calculate average profit per bar_range and trade_id\n",
    "df_range_avg = df.groupBy(\"trade_id\", \"bar_range\").agg(avg(\"profit\").alias(\"avg_profit\"))\n",
    "\n",
    "# Define a window partitioned by trade_id and ordered by bar_range\n",
    "window = Window.partitionBy('trade_id').orderBy('bar_range')\n",
    "\n",
    "# Create profit_lag column, which is the lagged cumulative average of avg_profit\n",
    "df_range_avg = df_range_avg.withColumn('cumulative_avg_profit', avg('avg_profit').over(window))\n",
    "df_range_avg = df_range_avg.withColumn('profit_lag', lag('cumulative_avg_profit').over(window))\n",
    "\n",
    "df_range_avg.orderBy('trade_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profit_lag = avg of avg_profit of bar_ranges before current\n",
    "# avg_profit = avg of profit of current bar_range\n",
    "df_new = df.join(df_range_avg, ['trade_id', 'bar_range'], 'left').fillna(0)\n",
    "df_new.orderBy(['trade_id', 'bar_range']).show(20)\n",
    "# df_new.select('trade_id', 'bar_range', 'profit_lag', 'avg_profit', 'profit').orderBy(['trade_id', 'bar_range']).show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "min_month = df.select(F.month(df.time_stamp)).groupBy().max().collect()[0][0]\n",
    "max_month = df.select(F.month(df.time_stamp)).groupBy().max().collect()[0][0]\n",
    "\n",
    "df = df.withColumn('bar_range', F.floor((df.bar_num - 1) / 10)) # starts from 1\n",
    "df = df.withColumn('bar_range', df.bar_range.cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "start_date = df.agg({\"time_stamp\": \"min\"}).collect()[0][\"min(time_stamp)\"]\n",
    "end_date = start_date + datetime.timedelta(6*365/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import datetime\n",
    "\n",
    "mape_list = []\n",
    "\n",
    "# Define your feature columns\n",
    "feature_columns = ['bar_range', 'direction', 'var12', 'var13', 'var14', 'var15', 'var16', \n",
    "                   'var17', 'var18', 'var23', 'var24', 'var25', 'var26', 'var27', 'var28', 'var34', 'var35', 'var36',\n",
    "                   'var37', 'var38', 'var45', 'var46', 'var47', 'var48', 'var56', 'var57', 'var58', 'var67',\n",
    "                   'var68', 'var78', 'profit_lag']\n",
    "\n",
    "# Initialize the VectorAssembler with the input and output column names\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "\n",
    "# Read the dataset\n",
    "df = df_new\n",
    "\n",
    "# Sort the dataframe by time_stamp\n",
    "df = df.orderBy('time_stamp')\n",
    "\n",
    "# Create a month_year column to group data\n",
    "df = df.withColumn('month_year', F.date_format('time_stamp', 'yyyy-MM'))\n",
    "\n",
    "# Assemble the features\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Cache the dataframe\n",
    "df.cache()\n",
    "\n",
    "# Set up the initial training period\n",
    "start_date = df.agg({\"time_stamp\": \"min\"}).collect()[0][\"min(time_stamp)\"]\n",
    "end_date = start_date + datetime.timedelta(6*365/12)\n",
    "\n",
    "# Initialize the regression evaluator\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"profit\", metricName=\"rmse\")\n",
    "\n",
    "# Set up the rolling window\n",
    "while end_date <= datetime.datetime(2015, 8, 3, 11, 15):\n",
    "    # Define the training data\n",
    "    train_df = df.filter((df['time_stamp'] >= start_date) & (df['time_stamp'] < end_date))\n",
    "\n",
    "    # Fit the model\n",
    "    lr = LinearRegression(featuresCol='features', labelCol='profit')\n",
    "    model = lr.fit(train_df)\n",
    "\n",
    "    # Define the test data (one month after the training period)\n",
    "    test_date = end_date + datetime.timedelta(365/12)\n",
    "    test_df = df.filter((df['time_stamp'] >= end_date) & (df['time_stamp'] < test_date))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mape = predictions.select(F.abs((F.col('profit') - F.col('prediction')) / F.col('profit')).alias('mape')).agg(F.mean('mape')).first()[0]\n",
    "    mape_list.append(mape)\n",
    "    print(f\"Current training period is {start_date.strftime('%Y-%m')} and {end_date.strftime('%Y-%m')}\")\n",
    "    print(f\"MAPE for prediction period {end_date.strftime('%Y-%m')} to {test_date.strftime('%Y-%m')}: {mape}\")\n",
    "\n",
    "    # Shift the training window\n",
    "    start_date = test_date\n",
    "    end_date = start_date + datetime.timedelta(6*365/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
